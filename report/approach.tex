\chapter{Multi-Modal Interaction-Aware Trajectory Optimization}
\label{text:approach}
In this chapter, an overview of the approach is provided. Starting in Section \ref{text:approach/formulation} the problem of socially aware motion planning is formulated formally, including the assumptions made within this project. 
\newline
Section \ref{text:approach/overview} which presents an overview of the full trajectory optimization pipeline. It is shown how the algorithm is built to allow to deal with general graph-like pedestrian behavior prediction models, with multi-modal, probabilistic outputs. Furthermore, the advantages of formulating socially aware motion planning as an optimization problem while utilizing a graph-based prediction model, e.g. deep learning-based models such as the Trajectron \cite{Ivanovic18}, over purely learned \cite{Chen2017} or purely optimization-based \cite{Berg2011} algorithms are illustrated. Finally, it is explained how the usage of a general-purpose, computation-graph based framework such as PyTorch \cite{pytorch} or TensorFlow \cite{tensorflow} can be harnessed for a highly modular, efficient, and versatile implementation.
\newline
The exact formulation of the optimization problem is further explained in Section \ref{text:approach/objective} and \ref{text:approach/constraint}. Beginning with the objective function both the goal-driven and the interaction-driven parts are illustrated, together with a derivation of their gradients. Furthermore, several designs of the specific objective function are discussed. The subsequent section addresses the design of the constraints used. Likewise, several constraint function designs are compared. 
\newline
To use the system in real-world applications it must comply with several requirements. While some of these have been tackled by the optimization design, e.g. feasibility of the control limitations of the robot or safety regarding robot-human interaction, the system underlies runtime limits. To achieve this goal several runtime improvements have been implemented, which are described in Section \ref{text:approach/runtime}.


\section{Problem Formulation}
\label{text:approach/formulation}
In this work we are interested in finding the optimal trajectory for a robot navigating among pedestrians on the two-dimensional plane. Let $\x_t = (x_t, y_t, \dot{x}_t, \dot{y}_t) \in \mathbb{R}^4 $ be the position and velocity of the robot, also referred as $ego$, at time $t$ and $\x_{0:N}$ a trajectory over multiple states $(\x_0, \x_1, \x_2, \hdots, \x_N)$. Within this work the robot is assumed to have double integrator dynamics. With control input $\u_t = (u_{x, t}, u_{y, t}) \in \mathbb{R}^2$, we have: 

\begin{equation}
\ddx_t = \u_t
\label{eq:robot_dynamics}
\end{equation}

For further simplification the robot's dynamics are assumed to be deterministic. 
The pedestrians (also referred as $ados$) follow single integrator dynamics. As pointed out in \cite{Ivanovic18} this is an intuitive choice "as a person’s movements are all position-changing, e.g. walking increases position along a direction, running does so faster". Other standard models for pedestrian dynamics such as Social Forces \cite{Helbling1995} however regard the pedestrian to be a double integrator, not a single integrator, and describe the forces acting on it introduced by other pedestrians, obstacles, etc. Having a reasonable fast reaction time and a large maximal acceleration in comparison to the robot both of these descriptions converge, so that the single integrator model is a good choice nonetheless.\footnote{As discussed in chapter \ref{text:experiments} the modular implementation allows us to use different pedestrian dynamics. When testing against other prediction environments non-single integrator dynamics are used, but most analysis described in this report relates to the single integrator model.} 

\begin{align}
\dxped[k]_t = \uped[k]_t
\label{eq:pedestrian_dynamics}
\end{align}

Each pedestrian's future trajectory is predicted as a probabilistic and multimodal using some model $\distmodel[]$ . Thereby,  let $\xped[k]_t \sim \dist[k]_t$ be the distribution of the pedestrian $k$s position at time $t$, with mean $\boldsymbol{\tilde{\mu}_t^k}$, variance $\boldsymbol{\tilde{\Sigma}_t^k}$ and mode-weights vector $\boldsymbol{\tilde{\pi}_t^k}$. The prediction is based on the past states of the robot $\x_{0:t}$ and of every pedestrian in the scene $\xped[j]_{0:t}$ including pedestrian $k$. Moreover, the function $\tilde{\Phi}$ is generally not shared overall pedestrian, but individual for each one.

\begin{align}
\xped[k]_t &\sim \dist[k]_t(\boldsymbol{\tilde{\mu}_t^k}, \boldsymbol{\tilde{\Sigma}_t^k}, \boldsymbol{\tilde{\pi}_t^k}) \\
\dist[k]_t &= \distmodel[k] (\x_{0:t}, \xped[0]_{0:t}, \hdots, \xped[K]_{0:t})
\end{align}

Similar to the robot the pedestrians are modeled as single point masses, both underlying speed bounds defined by the $L_2$ norm of their velocities. Furthermore both the robot and the pedestrians underly constraints for their minimal and maximal control effort. Thus, the sets of feasible control inputs $
\mathcal{U}$ and $\mathcal{\tilde{U}}$ respectively, are defined using the $L_1$-norm and the $L_2$-norm respectively:

\begin{align}
\mathcal{U} &= \{\u | \u \in \mathbb{R}^2, ||\u||_1 \leq u_{max}\} \\
\mathcal{\tilde{U}} &= \{\uped[] | \uped[] \in \mathbb{R}^2, ||\uped[]||_2 \leq \tilde{u}_{max}\} 
\label{eq:controls_bounds}
\end{align}
 
All actions are assumed to take place in a free-space, two-dimensional environment. 
\newline
Within project \project I want to find a robot trajectory $\x_{0:T}$ over some discrete-time horizon $N$ that trade-offs between minimizing the travel time from its current state to some goal state in $\mathcal{X}_f = \{\boldsymbol{g} | \boldsymbol{g} \in \mathcal{X} \}$ on the one side and the interference with the pedestrians in the scene on the other side, concerning its dynamic as well as safety boundaries. For further simplification, a perfect knowledge about the current and all past states of surrounding agents $\xped[k]_{0:t} \forall k \in [0, K]$ is assumed. 

\section{System Overview}
\label{text:approach/overview}
As shown in Figure \ref{img:information_flow} the problem can be divided into several subproblems, which also can be seen as modules of a trajectory optimization problem and therefore can be formulated independently from each other. 

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=\imgwidth]{images/placeholder.png}
\caption{Information-Flow-Diagram}
\label{img:information_flow}
\end{center}
\end{figure}

Formally, the solution to this problem can be represented as the following optimization problem (compare Problem \ref{eq:formulation}). In this work the initial time $t_0 = 0$ and the final time $t_f = T \cdot \Delta t$ is used for simple notation, although not required in general \cite{Wachter2006}. \\

\begin{problem} General \project optimization problem
\begin{equation}
\min_{\u_{0:T-1}} \quad w_{goal} J_{goal} + w_{int} \sum_{k=0}^K J_{int}^k
\end{equation}
\begin{align}
\textrm{s.t. } \quad & \x_{t+1} = f(\x_t, \u_t) & \forall t \in [0, T - 1] \\
& \xped[k] = \tilde{\Phi}_k(\x_{0:t}, \xped[i]_{0:t}) & \forall k \in [0, K], \forall t \in [0, T]\\
& g_{safety}(\x_{0:T}, \xped[k]_{0:T}) \leq 0 & \forall k \in [0, K] \\
& \x_t \in \mathcal{X} & \forall t \in [0, T]\\
& \u_t \in \mathcal{U} & \forall t \in [0, T]\\
& \x_0 \in \mathcal{X}_0
\end{align} 
\label{eq:formulation}
\end{problem}

The objective function is a weighted sum of a goad-driven $J_{goal}(\cdot)$ and some interaction-driven term $J_{int}(\cdot)$, which are further explained in Section \ref{text:approach/objective}. The constraints are a combination of dynamics constraints, safety constraints as well as initial and final state restrictions, which are discussed in Section \ref{text:approach/constraint}. Note that the terminal state $\x_T$ of the planned trajectory is not constrained to be in the terminal set $\mathcal{X}_f$. As stated in chapter \ref{text:introduction} within this project the safety and undisturbed gait of pedestrians is considered to be more important than reaching the goal state time-optimally. While the goal-driven objective $J_{goal}(\cdot)$ introduces an incentive to approach the goal state, constraining the terminal state to be in the terminal set would informally speaking, remove the possibility of detouring to not interfere a pedestrians movement. 

\subsubsection{Transcription Method} 
Trajectory optimization is a wide field with many different methodologies. Though the area can be roughly broken down into two categories, shooting and collocation \cite{Kelly2017}.\footnote{There are several other directions of separation possible such as the direct vs indirect methods, but I will focus on shooting vs collocation here. Further information about the taxonomy of trajectory optimization can be found in \cite{Kelly2017} and \cite{Chai2020}.} Shooting optimizes for the control inputs and unrolls them using a simulation environment to compute objective and constraint. Thereby the dynamics constraint $x = f(x, u)$ is intrinsically enforced. In comparison, collocation uses all controls and (!) states as decision variables, while constraining the system dynamics, and tries to solve the \ac{NLP} by approximating some function (e.g.\ Lagrange polynomials in orthogonal collocation). Due to the linear and cheap to compute dynamics, the availability of a simulation engine tightly bound to the optimization and since the constraints acting on both the path $\x_{0:T}$ itself and the controls $\u_{0:T}$ are comparably "simple", as illustrated in Section \ref{text:approach/constraint}, shooting is used within this project. Therefore the trajectory is split up into several segments, one for each discrete discretization time step, which makes solving problem \ref{eq:formulation} easier to be solved and more robust (multiple shooting) \cite{Betts1998}.

\subsubsection{Nonlinear optimization Solver} 
Since no further assumptions about the pedestrian trajectory prediction model $\tilde{\Phi}$ have been made, the optimization is non-linear and especially non-convex in general. As shown in \cite{Gould2003}\cite{Parkinson2018}\cite{Freund2004} there are a bunch of algorithms to deal with general non-linear optimization problems, such as line-search, trust-region, interior-point, generalized-reduced-gradient or sequential-quadratic-programming methods as well as combinations of these such as LOQO. However, not all of them apply to constrained problems such as Problem \ref{eq:formulation}. For constrained optimization problems \ac{SQP} and \ac{IPM} are the most popular ones, both having their advantages and drawbacks. While \ac{SQP} usually require fewer solver iterations and thus fewer function calls, they scale poorly with the number of constraints and do not guarantee that intermediate results are feasible \cite{Dehdari2013}\cite{Parkinson2018}. Although this can be an advantage in the case of computationally expensive constraints, it also leads to an infeasible solution when the algorithm is stopped for convergence. Additionally, compared to \ac{IPM} the computational time required by the solver itself is usually larger (e.g. as demonstrated in \cite{Dehdari2013}).
\newline
In project \project safety is induced by constraint, thus obtaining a feasible solution is crucial, also if the algorithm might be aborted before convergence due to runtime constraints. For this reason, an interior-point method has been used within the project. The interior-point method \ac{IPOPT} \cite{Wachter2006} has shown to be applicable in many robotics applications, and has shown to be valuable even in case of very tough runtime constraints such as solving the feedforward commands in high-performance automated driving at 50 Hz in \cite{Spielberge2019} or motion planning for legged robotics \cite{Winkler2018}. There it has been used in this project as well\footnote{From today's point of view it turned out, after tweaking the performance of the optimization core as best as possible, it turns out that the number of objective function calls is the bottleneck of the algorithm. Therefore comparing the capacity of the interior-point solver such as \ac{IPOPT} with \ac{SQP} solvers such as \ac{GuSTO} remains for future work.}.

% The NLP solver implements the following primal-dual methods for finding a local minimum: 1) interior point trust-region line-search algorithm 2)active-set trust-region line-search algorithm
%Both methods can solve small-, medium-, and large-scale optimization problems efficiently and robustly. These methods use exact first and second derivatives to calculate search directions. The memory requirements of both algorithms are reduced dramatically because only nonzero elements of matrices are stored. Convergence of both algorithms is achieved by using a trust-region line-search framework that guides the iterations towards the optimal solution. If a trust-region subproblem fails to provide a suitable step of improvement, a line-search is then used to fine tune the trust-region radius and ensure sufficient decrease in objective function and constraint violations.
%The interior point technique implements a primal-dual interior point algorithm in which barrier functions are used to ensure that the algorithm remains feasible with respect to the bound constraints. Interior point methods are extremely useful when the optimization problem contains many inequality constraints and you suspect that most of these constraints will be satisfied as strict inequalities at the optimal solution.
%https://documentation.sas.com/?docsetId=ormpug&docsetTarget=ormpug_nlpsolver_overview.htm&docsetVersion=14.3

\subsubsection{Implementation Details} 
The algorithm has been implemented in Python 3.7 using PyTorch \cite{pytorch}. Next to executing computations highly efficiently by vectorization and batching PyTorch's automatic differentiation framework allows computing gradients at low computational cost as well as modular, i.e.\ without the need to explicitly define its closed form. This is the core of prediction-based optimization, enabling to define objectives and constraints (such as $J_{int}$), which depend on a complex graph-based computation, and still being able to derive their gradient without costly and inaccurate numerical approximations. 
\newline 
Furthermore, the algorithm has been implemented highly modular, allowing to easily use pre-defined simulation environments, objective, constraint functions, or new solvers and switch between them without much knowledge about the underlying implementation. To pave the way for further projects based on this project it has been widely documented using Sphinx\footnote{Automated code documentation: https://www.sphinx-doc.org/en/master/}, for further information please visit the \href{https://simon-schaefer.github.io/mantrap/}{project's website}.  
\newline
The system's integrity has been verified by roughly 500 unit- and integration tests, secured over the code developed by the continuous integration framework CircleCI\footnote{Automated testing framework: https://circleci.com} and CodeCov\footnote{Testing coverage reports: https://codecov.io}.

\section{Objective Function Design}
\label{text:approach/objective}

\subsection{Goal Objective}
\label{text:approach/objective/goal}
The goal objective gives an incentive for the optimizer to choose a solution that targets the goal state. It simply consists of the squared L2-norm between each robot trajectory point and the goal state $\boldsymbol{g}$, normalized over the full planning horizon $N$:

\begin{equation}
J_{goal}(\x_{0:T}) = \frac{1}{T} \sum_{t = 0}^T (\x_t - \boldsymbol{g})^2
\label{eq:goal_unweighted}
\end{equation}

By normalization, the cost is independent of the length of the planning horizon $T$ and thus allows us to use the same weight $w_{goal}$ for different planning horizons.

\subsubsection{Horizon weighting}
Intuitively, as discussed in Chapter \ref{text:introduction} for socially aware navigation socially-aware objectives should be higher weighted than traditional control effort or travel time objectives. This is especially true at the beginning of the planning horizon. For this reason the a horizon dependent weighting $\lambda_t$ is introduced into the stage cost of the goal objective, which is small at the beginning of the horizon and large at its end: 

\begin{equation}
J_{goal}(\x_{0:T}) = \frac{1}{T} \sum_{t = 0}^T \lambda_t (\x_t - \boldsymbol{g})^2
\label{eq:goal_weighted}
\end{equation}

As shown later on this modification empowers a fast convergence to evasive movements of the robot, when necessary to avoid un-safe situations, while properties of $J_{goal}(\cdot)$ such as most importantly convexity remain unchanged. In Table \ref{table:goal_horizon_weighting} the convergence speed of both the "un-weighted" and the "weighted" objective formulation are compared over 100 runs in a simplified setup, with no pedestrian in the scene and random assignments of the robot's initial and goat state. Weighting the cost terms non-uniformly over the horizon enables finding "trade-offs" of a locally higher cost for a faster global convergence such as gaining speed in a non-goal-direction at the beginning of the horizon but leads to a smaller cost at the end of it. 

\begin{table}[!ht]
\begin{center}
\begin{tabular}{c|c|c}
 & Weighted & Un-Weighted \\
\hline
Avg. number of solving iterations & 9.58 & 9.64 \\
\hline
Avg. $95 \%$ decay of objective value & 5.86 & 6.64 \\
\end{tabular}
\caption{Comparison of key performance parameter of the optimization using either the "un-weighted" (Equation \ref{eq:goal_unweighted}) or the "weighted" (Equation  \ref{eq:goal_weighted}) goal objective formulation over 100 runs in a simplified environment setup. For further details please have a look into the example notebook: \href{https://github.com/simon-schaefer/mantrap/blob/master/examples/modules/goal.ipynb}{examples/modules/goal}.}.
\label{table:goal_horizon_weighting}
\end{center}
\end{table}

\subsubsection{Gradient}
Since the goal objective $J_{goal}(\x_{0:T})$ is only a function of the robot's planned trajectory and the goal state, its jacobian can be derived without further knowledge of the pedestrian prediction model, merely using the (known) robot dynamics. As described in Section \ref{text:approach/overview} the robot controls are optimized. Hence by applying the chain rule we get: 

\begin{equation}
\nabla J_{goal} = \pd{J_{goal}}{\u_{0:T-1}} = \pd{J_{goal}}{\x_{0:T}} \cdot \pd{\x_{0:T}}{\u_{0:T-1}}
\end{equation}

As demonstrated, the goal-objectives gradient can be derived by multiplying the objectives gradient with respect to the robot's trajectory with the gradient of the robot's trajectory with respect to its control inputs. Since the goal-objective directly depends on the trajectory, deriving the first term is straight-forward to derive. The second term $\delta \x_{0:T} / \delta \u_{0:T-1}$ is not trivial in general, because of the iterative structure of rolling out state trajectories based on dynamics and as the robot's dynamics $f(\cdot)$ can be arbitrary, however as described in Section \ref{text:approach/formulation} double integrator dynamics are assumed for the robot, so that the whole trajectory $\x_{0:T}$ can be expressed as function of the initial state $\x_0$ and the control inputs $\u_{0:T-1}$ only, as shown in equation \ref{eq:dynamics_stacked}. Then the term $\delta \x_{0:T} / \delta \u_{0:T-1}$ simplifies to a constant term:

\begin{align}
\pd{J_{goal}}{\x_{0:T}} &= \pd{}{\x_{0:T}} \frac{1}{N} \sum_{t = 0}^N (\x_t - \boldsymbol{g})^2 \\
&= \frac{2}{T} \begin{bmatrix} (\x_1 - \boldsymbol{g}) & \hdots & (\x_T - \boldsymbol{g}) \end{bmatrix}^T
\end{align}
\begin{align}
\pd{\x_{0:T}}{\u_{0:T-1}} &= \pd{}{\u_{0:T-1}} \begin{bmatrix} A \x_0 \\ A_n \x_0 + B_n \u_{0:T-1} \end{bmatrix} \\
&= \begin{bmatrix} \boldsymbol{0}_{n \times m} \\ B_n \end{bmatrix}
\label{eq:goal_gradient_dynamics}
\end{align}

with $A_n, B_n$ the stacked state-space description matrices as described in Section \ref{text:approach/runtime/unrolling}. 
\newline
Overall the goal objective and its gradient are very efficient, cheap to compute, having linear complexity with the length of the planning horizon $T$, and being independent of the number of pedestrians. Furthermore, it is strictly convex, which improves the optimization convergence speed. Therefore it is quite valuable for warm-starting the optimization algorithm, as further explained in Section \ref{text:approach/runtime/warm_starting}.


\subsection{Interaction Objective}
\label{text:approach/objective/interactive}
The interaction between robot and pedestrian itself is an abstract quantity and can thus hardly be used for optimization itself. To approach this problem in the following we consider a scene with the robot facing only one pedestrian, to then generalize the developed concept to interactions with multiple pedestrians.  
\newline
While it is hard to find a measure for the interaction between the robot and the pedestrian in the scene when regarding the problem in general, it can be simplified when a) focussing on one of the interacting agents and b) define the measure to vanish its value. Under these conditions, the problem can be re-formulated as decreasing the disturbance the robot induces on the pedestrian, which is a lot simpler to quantify than the abstract concept of interaction. To do so the un-conditioned\footnote{For the sake of brevity the term "un-conditioned" means not depend on the robot's state. However, the pedestrian's trajectory prediction is of course still conditioned on its state history as well as the states and state history of the other pedestrians in the scene, as described in Section \ref{text:approach/formulation}.} pedestrian's trajectory distribution $\distwo[]$ is computed, i.e. the distribution that would occur if no robot would be in the scene, and then compared to its actual conditioned trajectory distribution $\dist[]$.\footnote{When the prediction model requires the input of a robot trajectory, e.g. as an "input format" requirement of a neural network-based model, predicting the un-conditioned trajectory distribution might not be straight-forward. Using a re-trained un-conditioned model is not an option since factors causing a different distribution independent from the robot's trajectory might come into play. Thus, within the project a "pseudo"-robot is used which is located very far away from any pedestrian, hence minimizing its effect on the pedestrian's behavior.} With some general distance measure $\Delta(\cdot, \cdot)$ the interactive objective function given the robot's trajectory $\x_{0:T}$ is defined as:

\begin{equation}
J_{int}^k(\x_{0:T}) = \sum_{t=0}^T \Delta(\dist[k]_t, \distwo[k]_t)
\label{eq:objective_interaction}
\end{equation}

\begin{figure}[!ht]
\begin{center}
\begin{tikzpicture}

    \node[inner sep=0pt] (pedwo) at (-3,1)
    {\includegraphics[width=.05\textwidth]{images/walking.png}};    
    \draw [dotted, ultra thick, name path=A] (pedwo) to[out=180, in=0] node[above] {$\xpedwo[k]$} (-8, 1);
    

    \node[inner sep=0pt] (pedw) at (-3,-1)
    {\includegraphics[width=.05\textwidth]{images/walking.png}};
    \node[inner sep=0pt] (robot) at (-7,-1)
    {\includegraphics[width=.05\textwidth]{images/robot.png}};
    \draw [ultra thick, name path=B] (pedw) to[out=180, in=10] node[below, sloped] {$\xped[k]$}(-8,-3);
    
    \draw[thick, decorate, decoration={brace, amplitude=20pt}] (-1.5,2) -- (-1.5,-2);

    \node[inner sep=0pt] (ped) at (5,0.5)
    {\includegraphics[width=.05\textwidth]{images/walking.png}};
    
    \draw [dotted, ultra thick, name path=A] (ped) to[out=180, in=0] node[above] {$\xpedwo[k]$} (0, 0.5);
    \draw [ultra thick, name path=B] (ped) to[out=180, in=10] node[below, sloped] {$\xped[k]$}(0,-1.5);
        
    \tikzfillbetween[of=A and B]{blue, opacity=0.2};
    \node[] (D) at (1, -0.3){$D_{int}$};
    
\end{tikzpicture}
\end{center}
\caption{Interactive measure in case of deterministic and uni-modal pedestrian trajectory predictions, which trivially is the area enclosed by the un-conditioned (upper) and the conditioned (middle) trajectory.}
\label{img:ado_w_wo_distance_trajectory}
\end{figure}

If the pedestrian's trajectory prediction would be deterministic and uni-modal this measure simply breaks down to the distance between both trajectories which is the area enclosed by both trajectories in continuous time, as shown in Figure \ref{img:ado_w_wo_distance_trajectory}, or a sum of distance per time-step in discrete time. For probabilistic and multi-modal predictions computing the distance between both distributions is more difficult, especially when demands such as computational cost and differentiability (to be used in an optimization) have to be factored in. 

\begin{figure}[!ht]
\begin{center}
\begin{tikzpicture}

    \node[inner sep=0pt] (ped) at (5,0)
    {\includegraphics[width=.05\textwidth]{images/walking.png}};
    \node[inner sep=0pt] (robot) at (0,0)
    {\includegraphics[width=.05\textwidth]{images/robot.png}};
       
    \draw [dotted, ultra thick, name path=A] (ped) to[out=180, in=0] (0, 2) node[above, sloped] {$\xpedwo[k]$}  to[out=180, in=0] (-4, 0);
    
    \draw [ultra thick, name path=B] (ped) to[out=180, in=10] (0,-2) node[below, sloped] {$\xped[k]$} to[out=180, in=0] (-4, 0);
    
\end{tikzpicture}
\end{center}
\caption{Interactive measure in case of deterministic and uni-modal pedestrian trajectory predictions, which trivially is the area enclosed by the un-conditioned (upper left) and the conditioned (upper right) trajectory.}
\label{img:int_acceleration_reason}
\end{figure}

The correlation between the acceleration carried out on a passenger and its comfort is widely known, e.g. for the driving use case \cite{Hoberock1976}. Therefore assuming that the same measure applies for correlation between the acceleration the pedestrian itself has to exert (e.g. to evade a dynamic obstacle) and its comfort is likely. Therefore, instead of contrasting the positional distributions it might be valuable to use the accelerational distributions. Figure \ref{img:int_acceleration_reason} shows a possible scenario, in which the presence of the robot affects the pedestrian such that the trajectory prediction is mirrored. When the robot is static and when no other pedestrian is close, both trajectories are equally safe (if the robot is static)  and "comfortable" for the pedestrian and are equal in length, so there is no reason to chose one above the other. However, due to the large enclosed area, a purely position-based distance metric would be non-zero by far, while an acceleration-based distance metric would be zero since the velocities of the pedestrian-only change their sign, not their absolute value.

\subsubsection{Kullback-Leibler Divergence}
A commonly used metric for expressing the distance between two distributions is the Kullback-Leibler Divergence $D_{KL}$, which determines the distance between  some distribution $q$ and another distribution $p$ as:

\begin{equation}
D_{KL} = \int_x q(x) log \frac{q(x)}{p(x)} dx    
\end{equation}

While $D_{KL}$ is a well-defined, is capable to be used in optimization procedures and is therefore commonly used in many applications, such as generative deep learning models \cite{Goodfellow2014}\cite{Salzmann2020} (similarly the Jenson-Shannon Divergence), it is not analytically defined for some "complex" distributions such as \ac{GMM}, which is the output distribution format of Trajectron \cite{Ivanovic2018}. Methods to approximate the KL-Divergence for \ac{GMM}s have been discussed in \cite{Cui2015} and embrace Monte Carlo sampling, signature quadratic form distance \cite{Beecks2011} and several more which however are not computationally feasible for an online application, especially since its gradient has to be computed. Other methods simplify the real \ac{GMM} to a single Gaussian, by a weighted average over its parameters, which is not guaranteed to be a meaningful distribution and loses the advantages of predicting multi-modal distributions in the first place. 
\newline
Regarding the Trajectron \cite{Ivanovic2018} as prediction model, some intermediate distribution might be used for comparison instead of the output distribution, such as the categorical distribution in its latent space\footnote{In fact, the Trajectron's latent space could not be used as a basis for the interactive objective function anyway, since it does not depend on the robot's trajectory. However, it might be used to assess the similarity between scenarios, which will be discussed in Section \ref{text:approach/runtime/warm_starting}.}. Although this approach might give rise to use widely used distance measures such as $D_{KL}$ it would impede tractability and generality of the overall framework, as it would have to be re-defined for every other prediction model. 

\subsubsection{Sample-Wise Distance}
Next to taking the full trajectory distribution into account instead of the distance between the un-conditioned $\distwo[]$ and the conditioned trajectory distribution $\dist[]$ can be approximated by computing the expected value over sample pairs. Then the distance measure breaks down to a weighted sum over $L_2$-norms for each discrete time-step within the time horizon and for every trajectory pair, which are efficient to compute. 

\begin{equation}
D_{int}^{sp} = \sum_{samples} \sum_T ||\xpedwo[s]_t - \xped[s]_t||_2
\end{equation}

As previously described it might be more meaningful to compare accelerational instead of positional data. Using samples instead of the full distribution allows us to efficiently compute the velocity and accelerations of the pedestrian at each predicted time-step numerically, using central difference expressions. 

\begin{equation}
D_{int}^{sa} = \sum_{samples} \sum_T ||\ddxpedwo[s]_t - \ddxped[s]_t||_2
\end{equation}

Although quite intuitive it turns out that a sample-wise objective is hard to optimize. This has two pre-dominant reasons: Firstly, it intrinsically relies on a trade-off between computational feasibility (to compute many times per second for an online optimization) and capability to capture the properties of the underlying real distributions sufficiently well. Secondly, when randomly drawn samples are used, stochasticity is introduced into the objective function, which might lead to a different objective value even when evaluated with the same input. When the distribution's means is used instead, the distribution's uncertainty is disregarded. 

\subsubsection{Trajectory Projection}
Combining computational efficiency and the capability of representing a measure based on the full distribution is hard, as demonstrated in the examples above. However, by exploiting that the predicted distribution $\xped[]_t \sim \dist[]_t$ is a continuous, well-defined distribution with infinite support.\footnote{Although not all distributions have infinite support, these properties surely hold for the most commonly used ones in the area of pedestrian prediction such as Gaussians, \ac{GMM}s \cite{Salzmann2020} or non-closed form distribution such as SGAN \cite{Gupta2018}.} Then \ref{eq:objective_interaction} can be re-defined as the probability of the the un-conditioned distribution $\distwo[]_t$ with respect to the conditioned distribution $\dist[]_t$, which is equivalent to the integral over the product distribution $\int \int \distwo[]_t \cdot \dist[]_t \, dxdy$.
\newline
For two \ac{GMM}s the distribution product is not analytically defined though, and would involve numerically solving an \ac{ODE} \cite{Schrempf2005} or multi-scale sampling \cite{Ihler2003}. Therefore as simplification not the full un-conditioned distribution is taken into account but only its mean value $\mathbb{E}[\distwo[]_t]$, and weighted by the conditioned mode importance vector:\footnote{A derivation of the exact objective formulation for a \ac{GMM} as underlying distribution can be found in the appendix.} 

\begin{equation}
J_{int}^k = - \sum_{t=0}^T \mathbb{E}_{\xped[k] \sim \dist[k]} \log p(\, \mathbb{E}[\distwo[k]_t] \, | \, \xped[k]_t, \x_t)
\label{eq:objective_interact_prob}
\end{equation}

To deal with reasonable large values, compared to the other objective functions, and for independence of gradients (sum not product), instead of the \ac{PDF} $p = pdf(x, y)$ its logarithmic value $\log p$ probability is used. Since the product probability should be maximized while the optimization stated in Problem \ref{eq:formulation} seeks the minimum, the expectation value's negative is used.
\newline
Equation \ref{eq:objective_interact_prob} is efficient to compute as it can be batched over the full length of the trajectory and all modes. Also it uses the full distribution and has a unique global minimum when the means of both distribution are identical (at least for Gaussian-like distributions such as \ac{GMM}s). In fact \ref{eq:objective_interact_prob} is similar to the \ac{ELBO} loss of the Trajectron loss function (compare Equation \ref{eq:trajectron_loss}), which shows that the term is suitable for general optimization, especially with respect to the Trajectron model itself. However $J_{int}$ is generally applicable to all prediction models, that output a probabilistic distribution, independent from multi-modality.


\section{Constraint Function Design}
\label{text:approach/constraint}

\subsection{Dynamics Constraints}
\label{text:approach/constraint/dynamics}
The dynamics constraints subside all constraints in problem \ref{eq:formulation} that are not directly related to safety, i.e. 

\begin{align}
\label{eq:constraint_f}
&\x_{t+1} = f(\x_t, \u_t) \qquad& \forall t \in [0, T - 1] \\
\label{eq:constraint_x}
&\x_t \in \mathcal{X} & \forall t \in [0, T]\\
\label{eq:constraint_u}
&\u_t \in \mathcal{U} & \forall t \in [0, T]\\
\label{eq:constraint_x0}
&\x_0 \in \mathcal{X}_0
\end{align}

Using the shooting trajectory optimization paradigm the decision variables are the robot's control inputs  $\u_t \in \mathcal{U}$ so that the states $\x_t$ are obtained by unrolling the controls starting at some initial state $\x_0 \in \mathcal{X}_0$ and iteratively using the robot's dynamics $f(\cdot)$. \ac{IPOPT} on the other side solves a general \ac{NLP} of the form \cite{Wachter2006} \\

\begin{problem}{General IPOPT problem formulation}
\begin{align}
\min_{x \in \mathbb{R}^n} \quad & f(x) \\
\textrm{s.t. } \quad & g^L \leq g(x) \leq g^U \\
& x^L \leq x \leq x^U 
\end{align}
\label{eq:formulation_ipopt}
\end{problem}

As indicated in Section \ref{text:approach/formulation} the bounds of the controls $\u_t$ have a shape, as the decision variable $x$ in problem \ref{eq:formulation_ipopt}. Therefore \ref{eq:constraint_u} is implicitly posed, without further ado, just by using the robot's control input bounds as bounds of the decision variable. Also the constraints \ref{eq:constraint_f} and  \ref{eq:constraint_x0} are satisfied by the problem design, using the shooting method, leaving merely the constraint \ref{eq:constraint_x} to be explicitly defined. The state $\x$ of the robot incorporates both, its position and its velocity. While the prediction models, objectives and constraints depend only  relative measures regarding the agent's positions and therefore the positional subset of $\mathcal{X}$ can be safely assumed as being unbounded ($\mathcal{X}_{pos} = \mathbb{R}^2$). Due to the speed boundaries that are imposed on the robot ($||v||_1 \leq v_{max}$, comp. Section \ref{text:approach/formulation}), in order for the solution to be feasible a maximal speed constraint has to be established:

\begin{equation}
\x_t \in \mathcal{X} \Rightarrow -v_{max} \leq g_{v_{max}}(\x_t) = \dot{\x}_t \leq v_{max} \quad \forall t \in [0, T]
\label{eq:constraint_v_max}
\end{equation}

Computing the jacobian for constraint \ref{eq:constraint_v_max} is straight-forward using chain rule and exploiting the linear robot dynamics (as above when deriving the goal objective's gradient in equation \ref{eq:goal_gradient_dynamics}):  

\begin{align}
\nabla g_{v_{max}} &= \pd{g_{v_{max}}}{\u_{0:T-1}} = \pd{g_{v_{max}}}{\x_{0:T}} \cdot \pd{\x_{0:T}}{\u_{0:T-1}} \\
\Rightarrow \pd{g_{v_{max}}}{\x_{0:T}} &= \begin{bmatrix} \pd{g_{v_{max}}^1}{\x_{0:T}} & \hdots & \pd{g_{v_{max}}^T}{\x_{0:T}}\end{bmatrix}^T  \\
&= \begin{bmatrix} 
0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & \hdots & 0 \\ 
0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & \hdots & 0 \\  
0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & \hdots & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & \hdots & 0 \\ 
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \hdots & 1 \end{bmatrix} \\
\Rightarrow \pd{\x_{0:T}}{\u_{0:T-1}} &= \begin{bmatrix} \mathbf{0}_{n \times m} \\ B_n \end{bmatrix}
\end{align}
 

\subsection{Safety Constraint}
\label{text:approach/constraint/safety}
As pointed out in Chapter \ref{text:introduction} one of the main advantages of an optimization-based approach for robot navigation in human crowds is the possibility to ensure safety, namely by posing safety guarantees in form of constraints to the optimization problem. Infusing safety into a dynamical and probability system is not trivial, and widely dominated by approaches using chance-constraints. 

\subsubsection{Chance Constraints}
Sampling-based methods relying on the Bernstein approximation \cite{Calafiore2005}, on scenario planning approach \cite{Bemporad1999} or on Monte Carlo sampling \cite{Hong2011}\cite{Janson2015} as well as dynamic-programming based approaches for discrete space \cite{Yin-LamChow2013}\cite{Ono2015} \cite{Chow2015a} have been purposed. While these methods work for general distributions but usually lack are too slow for an online optimization approach, shrinking the space of distributions can improve the performance, such as in \cite{Calafiore2006}\cite{Carvalho2014}\cite{Blackmore2009}\cite{Blackmore2011}. 
\newline
However as stated in \cite{Lew2020} these approaches are slow for a large number of obstacles. In the context of \project the pedestrians ("obstacles") are assigned to multi-modal distributions. While the most of the previously mentioned work on sampling-free chance constraints focus on uniformly radial distributions, e.g. Gaussians, none of them takes multi-modality into account. To do so, a chance constraint could be formulated individually for each mode of a multi-modal distribution, and weighed by multiplying the mode's importance weight $\pi_m$, might be a feasible approach. For each pedestrian $k$, mode $m$, prediction time-step $t$ and probability threshold $p$ the chance-constraint would then be:

\begin{equation}
\pi_t^{k,m} \cdot Pr(g_t^{k,m}(x)\leq 0) \geq p
\end{equation}

However, doing so entails repeatedly calling the prediction model $\distmodel[]$. Constraining the sum or product of the mode-wise chance constraints instead, by using Boole's inequality, would be very conservative. In any way using chance-constraints the evaluation of the set of resulting constraints would be at least linear with the number of pedestrians, the number of modes and the length of the planning horizon. While the constraint evaluation can be batched, programmatically each constraint has to be differentiated individually (as required by the PyTorch automatic differentiation engine \cite{pytorch}), which is the main bottleneck of the whole trajectory optimization problem. 

\subsubsection{Hamilton-Jacobi Reachability} 
Reachability analysis deals with the problem of a two-person, zero-sum differential game, specifically it tries to solve the question how to react if there is another player that interferes with the fulfillment of the robot's objective by optimizing the joint system state, assuming that the counter-player always counters the robot's action optimally, i.e. assuming the worst case disturbance $\boldsymbol{d} = \beta[\boldsymbol{u}]$ \cite{Pavone2020}. Under these conditions the optimal control strategy can be derived by maximizing: \\

\begin{equation}
V(x(t), t) = \min_{\beta[\boldsymbol{u}](\cdot)} \max_{\boldsymbol{u}(\cdot)} \left[ \int_t^0 g(\boldsymbol{x}(\tau), \boldsymbol{u}(\tau), \boldsymbol{d}(\tau)) \, d\tau + h(\boldsymbol{x}(0)) \right]
\label{eq:j_reachability}
\end{equation}

with $(\boldsymbol{x}(\tau), \boldsymbol{u}(\tau), \boldsymbol{d}(\tau))$ describing the joint system with dynamics $f(\boldsymbol{x}, \boldsymbol{u}, \boldsymbol{d})$. While solving for the robot's optimal control strategy $\boldsymbol{u}_{opt}$ in open-loop (also \textit{forward} reachability) is computationally cheap, but leads to overly conservative and hence un-realistic estimates, since the counter-player knows the entire policy $\boldsymbol{u}_{opt}$ in advance and can re-act regardingly, \ac{HJR} (also \textit{backward} reachability) addresses the problem of maximizing Equation \ref{eq:j_reachability} in closed-loop, i.e. by allowing the robot to adapt its control policy at each time-step. As proven in \cite{Pavone2020} this is equivalent as solving the Hamilton-Jacobi-Isaacs differential equation with boundary condition: \\

\begin{problem}{General \ac{HJR} problem}
\begin{align}
\pd{V}{t} + \max_{\boldsymbol{u}}  \min_{\boldsymbol{d}} &\left[g(\boldsymbol{x}, \boldsymbol{u}, \boldsymbol{d}) + \nabla V^T f(\boldsymbol{x}, \boldsymbol{u}, \boldsymbol{d}) \right] = 0 \\ 
V(\boldsymbol{x}, 0) &= h(\boldsymbol{x}) \\
\Rightarrow \boldsymbol{u}_{opt} &= \arg \max_{\boldsymbol{u}}  \min_{\boldsymbol{d}} \nabla V^T f(\boldsymbol{x}, \boldsymbol{u}, \boldsymbol{d})
\label{eq:hjr_problem_u}
\end{align}
\label{eq:hjr_problem}
\end{problem}

Solving Problem \ref{eq:hjr_problem} has been examined for several scenarios and applications over the recent years, as in \cite{Dhinakaran2017}\cite{Margellos2009}\cite{Chen2017b}. In opposite to forward reachability \ac{HJR} finds the non-overly conservative avoidance maneuvers, "stemming from its equivalence to an exhaustive search over the joint system dynamics", while still being flexible with respect to the system dynamics, as depicted by \cite{Leung2020}. To solve \ref{eq:hjr_problem} the value function $V(\cdot)$, as well as its gradient, are computed for every cell of a $n$-dimensional grid that discretizes the joint state $\boldsymbol{x}(\tau)$ and for some discrete time horizon (including $\tau \rightarrow \infty$). 
\newline
\cite{Leung2020} presents a safety-constraint, based on the resulting value function's gradient, by defining the set of safety-preserving robot control actions as:

\begin{equation}
\mathcal{U} (\x_{rel}) = \{ \u: \min_{\boldsymbol{d}} \nabla V (\x_{rel})^T f_{rel}(\x, \boldsymbol{d}) \leq 0 \}
\end{equation}

Since the value function $V$ and its gradient $\nabla V$ are computed offline, the constraint is highly efficient to employ online. As the value function has been computed as backward reachable tube, the 

\subsubsection{\ac{HJR} in single pedestrian scenario}
In \project problem \ref{eq:hjr_problem} can be understood as a differential game between the robot and the pedestrians. At first, a game between the robot and one pedestrian only is considered, resembling a run-and-catch game, in which the pedestrian, the "disturbance", takes actions to decrease the distance between robot and pedestrian while the robot counter acts on that. Thereby, the coupled system state describes the direction-wise distance between robot and pedestrian $\x - \boldsymbol{d} = \x - \xped[]$ as well as the robot's velocity $\dx$. As previously pointed out the pedestrian is modeled as a single integrator, so that its velocity $\dxped[]$ is a control input rather than a state variable. The coupled system dynamics $f_R$ are: 

\begin{equation}
f_rel(\x, \xped[]) = 
\begin{bmatrix} 
\dot{x} - \dot{\tilde{x}} \\  
\dot{y} - \dot{\tilde{y}} \\  
\ddot{x} \\
\ddot{y} 
\end{bmatrix} = 
\begin{bmatrix} 
\dx - \dxped[] \\ 
\u
\end{bmatrix}
\label{eq:hj_dynamics}
\end{equation}

By solving Equation \ref{eq:hjr_problem_u} for the given coupled system dynamics, the robot's optimal control action $\u_{opt}^{HJR}$, regarding Problem \ref{eq:hjr_problem}, is determined as by maximizing:

\begin{equation}
\arg \max_{\u} \nabla V \cdot f(\x, \xped[]) = \arg \max_{\u} (dV_3 u_x + dV_4 u_y) = 
\begin{cases}
u_{max} & \text{if } dV_i \geq 0 \\
u_{min} & \text{if } dV_i < 0	
\end{cases}
\label{eq:hj_u_opt}
\end{equation}

Similarly the worst-case disturbance can be derived as being the disturbance extremum which is opposite to the sign of the value function's gradient. For this specific  dynamics function the min- and max-operator are exchangeable in order (minimax-theorem), so that the optimal disturbance and control can be regarded independently.
\newline
Figure \ref{img:hj_value_function} displays two-dimensional slices of the four-dimensional value function. Thereby the left plot shows the one-dimensional case, i.e. $y = 0$ and $v_y = 0$. In particular, it is worth to mention that the value function is mostly linear. Note that the asymmetry with respect to $x = 0$ originates from the differences in maximal and minimal speed between robot and pedestrian, respectively.  In fact the pedestrian is assumed to have a larger maximum speed compared to the robot, as further discussed in Chapter \ref{text:experiments}. Besides, the value function is symmetric in direction $x$ and $y$, as demonstrated in the right plot of Figure \ref{fig:hj_value_function}, which can easily be derived from Equations \ref{eq:hj_dynamics} and \ref{eq:hj_u_opt}.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=\imgwidth]{images/hj_value_function.png}
\caption{2D slices of 4D \ac{HJR} value function: Position and velocity for one-dimensional case ($y = v_y = 0$, left) and two-dimensional static case ($v_x = v_y = 0$, right)}
\label{img:hj_value_function}
\end{center}
\end{figure}

The value function and its gradient can be computed using the \textit{helperOC} Matlab toolbox \cite{Bansal2017}.\footnote{For more information please visit my toolbox's fork on GitHub \href{https://github.com/simon-schaefer/HJReachibility}{HJReachability Toolbox}}

% Quellen: neural network representation of value function to make it differentiable (Karen: reachability, Kyle David Julian: network representation to make sure all the corner cases are met)
% special case => current state is infeasible, then convert to cost of the high weight

% Constraining value function > 0 instead of gradient since gradient always worse (pedestrian faster than robot) ? => yes
% Coupling: Just the intersection as weakly coupled system with combining element the robot’s state ? => yes
% How to deal with multiple control actions ? Just constraint initial control action ? => just initial control step with horizon = planning horizon or longer
% LWPR: infinite runtime ==> linear interpolation (like Karen’s paper) ==> buffer around V > 0 like V > 0.2 to account for interpolation errors

\section{Runtime optimization}
\label{text:approach/runtime}

\subsection{Efficient Trajectory Unrolling}
\label{text:approach/runtime/unrolling}
One of the most promising directions for decreasing the runtime of a trajectory optimization algorithm is to decrease the runtime of the objective and constraints evaluation as well as their gradients. As shown in sections \ref{text:approach/objective} and \ref{text:approach/constraint} the majority of these optimization modules depends on the robot's trajectory $X_R$, rather than on its control inputs $U_R$. However since control inputs are optimized, as described in Section \ref{text:approach/overview}, a computationally efficient transformation from controls and initial state to the trajectory is key for making the trajectory optimization real-time feasible.
\newline
Fortunately, we assumed that the robot follows double integrator dynamics, which are linear and markovian. Hence, they can be expressed as the following: 

\begin{equation}
\x_{t+1} = A \x_t + B \u_t
\label{eq:dynamics}
\end{equation}

\begin{minipage}{0.5\textwidth}
$$A = \begin{bmatrix} 1 & 0 & \delta t & 0 \\ 0 & 1 & 0 & \delta t \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{bmatrix}$$
\end{minipage}
\begin{minipage}{0.5\textwidth}
$$B = \begin{bmatrix} 0 & 0 & \delta t & 0 \\ 0 & 0 & 0 & \delta t \end{bmatrix}$$
\end{minipage}

Due to the linear (not linearized !) dynamics $\delta t = \Delta t$ can be safely used. In order to fully "unroll" a trajectory the linear dynamic equation \ref{eq:dynamics} would be to be applied iteratively for the length of the trajectory. Since the dynamics themselves are linear (not just a first-order approximation) this computation can be further batched and on this way, speeded up. In fact the full trajectory can be derived merely based on the initial state $\x_0$ and the control input matrix $\u_{0:T}$, as shown in the following:

\begin{align}
\x_1 &= A \x_0 + B \ u_0 \\
\x_2 &= A \x_0 + B \ u_1 = A^2 \x_0 + A B \ u_0 + B \ u_1\\ 
\hdots &= \hdots \\
\begin{bmatrix} \x_1 \\ \x_2 \\ \vdots \\ \x_n \end{bmatrix} &= \underbrace{\begin{bmatrix} A \\ A^2 \\ \vdots \\ A^n \end{bmatrix}}_{\substack{A_n}} \x_0 + \underbrace{\begin{bmatrix} B & 0 & \hdots & \hdots & 0 \\ AB & B & 0 & \hdots & 0 \\ \hdots & \hdots & \hdots & \hdots & \hdots \\ A^{n-1} B & A^{n-2} B & \hdots & \hdots & B \end{bmatrix}}_{\substack{B_n}} \begin{bmatrix} \u_0 \\ \u_1 \\ \vdots \\ \u_{n-1} \end{bmatrix}
\end{align}

In summary, we get the following (also linear) expression for computing the full robot trajectory at once. As demonstrated in \href{https://github.com/simon-schaefer/mantrap/blob/master/examples/tools/timing.ipynb}{examples/tools/timing} using the fully batched formulation speeds up the trajectory "unrolling" by about a factor of 30.  

\begin{equation}
\x_{1:n} = A_n \x_0 + B_n \u_{0:T-1}
\label{eq:dynamics_stacked}
\end{equation}

\subsection{Warm Starting}
\label{text:approach/runtime/warm_starting}
It is widely known that warm starting an optimization can be very beneficial for its convergence speed for several optimization algorithms, e.g. shown in \cite{Banerjee2020} for \ac{GuSTO} or for \ac{IPOPT} in \cite{Shahzad2010}, \cite{John2008} or \cite{Spielberge2019}.
\newline
Within project \project the algorithm has been warm-started by solving the optimization problem posed in problem \ref{eq:formulation} while ignoring all objectives and constraints that relate to pedestrians, i.e. by solving the following optimization problem: \\

\begin{problem}{Simplified \project optimization problem for warm-starting}
\begin{align}
\min_{\u_{0:T-1}} \quad & J_{goal}(\x_{0:T}) \\
\textrm{s.t. } \quad & \x_{t+1} = f(\x_t, \u_t) & \forall t \in [0, T - 1] \\
& \x \in \mathcal{X} & \forall t \in [0, T]\\
& \u_t \in \mathcal{U} & \forall t \in [0, T]\\
& \x_0 \in \mathcal{X}_0
\end{align} 
\label{eq:formulation_warm_starting}
\end{problem}

Since the simplified optimization problem in problem \ref{eq:formulation_warm_starting} does not depend on the pedestrian dynamics model $\tilde{\Phi}$ it is much easier and more efficient to solve, being a convex quadratic program. \\

\begin{figure}[!ht]
\begin{center}
\begin{tikzpicture}

    \node (R) at (0, 0) [circle, shade, draw] {Robot};
    \node (G) at (8, 4) [circle, shade, draw] {Goal};
    \node (P1) at (2, 3) [circle, fill=orange, draw] {$P_1$};
    \node (P2) at (8, 1) [circle, fill=yellow, draw] {$P_2$};
    
    \tkzDefPointBy[projection=onto R--G](P1)  \tkzGetPoint{P1m}
    \tkzDefPointBy[projection=onto R--G](P2)  \tkzGetPoint{P2m}
    
    \draw[->, dotted, very thick] (R) to node[above] {} (G);
    \draw (P1m) -- (P1) node[midway, sloped, above] {$\mu_{P1}$};
    \draw (R) -- (P1m) node[midway, sloped, above] {$\eta_{P1}$};
    \draw (P2m) -- (P2) node[midway, sloped, above] {$\mu_{P2}$};
    \draw (R) -- (P2m) node[midway, sloped, below] {$\eta_{P1}$};
    
\end{tikzpicture}
\end{center}
\label{img:robot_goal_encoding}
\end{figure}

%Does the pre-computed set contain every possible scenario? No, but just warm-start so not required, there will be a constrained optimization afterward anyway

\subsection{Attention}
\label{text:approach/runtime/filtering}
